FROM apache/airflow:2.9.3-python3.10

# 1) Install the Spark provider using Airflow constraints (keep this as-is)
RUN pip install --no-cache-dir \
    "apache-airflow-providers-apache-spark<5.0.0" \
    --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.9.3/constraints-3.10.txt"

# 2) Install pyspark pinned to match your Spark cluster (DO NOT use Airflow constraints here)
#    Pin py4j too (Spark 3.5.x uses py4j 0.10.9.7)
RUN pip install --no-cache-dir \
    "pyspark==3.5.0" \
    "py4j==0.10.9.7"

USER root
RUN apt-get update && apt-get install -y \
    openjdk-17-jdk \
    procps \
    iputils-ping \
    netcat-openbsd \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-arm64
ENV PATH="$JAVA_HOME/bin:$PATH"

USER airflow
